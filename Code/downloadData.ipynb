{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================== Download Stock Price and Fundamental Data ===================================================== #\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import nasdaqdatalink as dtalink\n",
    "import quandl\n",
    "import pandas_datareader.data as pdr # to access FRED\n",
    "from pandas_datareader.famafrench import  get_available_datasets\n",
    "from functools import reduce\n",
    "\n",
    "# import technical indicators\n",
    "from talib_technical_indicators import  get_tech_indicators,  get_pattern_recognitions\n",
    "\n",
    "# import the talib library to compute technical indicators\n",
    "import talib\n",
    "\n",
    "# enter the FRED API key\n",
    "fred_api_key = os.getenv(\"./API/.fred_apikey\")\n",
    "\n",
    "# enter the NASDAQ Data Link API key\n",
    "# dtalink.ApiConfig.verify_ssl = False\n",
    "dtalink.read_key(\"./API/.dtalink_apikey_mc\")\n",
    "\n",
    "# Define a function to download stock, bond, inflation data\n",
    "def get_stock_data(ticker: str, start_date: str, \n",
    "                                end_date: str, \n",
    "                                out_dir = 'e:/Copy/SCRIPTS/Forecast_Stocks/Data/'):\n",
    "    \"\"\"\n",
    "        INPUT\n",
    "            ticker: a Yahoo! Finance stock ticker\n",
    "            start_date, end_date: start and end dates of data\n",
    "            out_dir: a path to the directory where data will be stored\n",
    "        OUTPUT\n",
    "            a dataframe of all variables to be downloaded\n",
    "    \"\"\"\n",
    "    stock = yf.Ticker(ticker)\n",
    "    stock_df = stock.history(start=start_date, end=end_date).tz_localize(None)\n",
    "    stock_df['return'] = (stock_df.Close + stock_df.Dividends) / ( stock_df.Close.shift(1) + stock_df.Dividends.shift(1) ) - 1.\n",
    "    stock_df['log_return'] = np.log(stock_df['return'] + 1.)\n",
    "    stock_df['price'] = stock_df.Close\n",
    "    stock_df['Dividends_ffill'] = stock_df.Dividends.where(stock_df.Dividends > 0, np.nan).fillna(method = 'ffill', axis = 0)\n",
    "    # stock_df['dp'] = stock_df.Dividends_ffill / stock_df.Close\n",
    "    stock_df.drop(columns = ['Dividends_ffill'], inplace = True)\n",
    "\n",
    "    # do the log-transform of volumes\n",
    "    stock_df['log_volume'] = np.log(stock_df['Volume'])\n",
    "\n",
    "    # get risk-free rate from Fama & French's data library\n",
    "    ff_factors_daily_df = pdr.DataReader('F-F_Research_Data_Factors_daily', 'famafrench',  start = start_date, end = end_date, api_key = fred_api_key)\n",
    "    stock_df = pd.merge(stock_df, ff_factors_daily_df[0].RF / 100, how = 'inner', left_index=True, right_index=True).drop_duplicates(keep='first')\n",
    "\n",
    "    # get 3-month T-Bill rate from FRED\n",
    "    DGS3MON_df = pdr.DataReader('DGS3MO', 'fred', start = start_date, end = end_date, api_key = fred_api_key)\n",
    "    DGS3MON_df.rename(columns = {'DGS3MO': 'RF'}, inplace = True)\n",
    "    stock_df = pd.merge(stock_df, DGS3MON_df.RF / 100, how = 'inner', left_index=True, right_index=True).drop_duplicates(keep='first')\n",
    "\n",
    "    # get the ICE BofA US Corporate Index Total Return Index\n",
    "    corp_bond_df = pdr.DataReader('BAMLCC0A0CMTRIV', 'fred', start = start_date, end = end_date, api_key = fred_api_key)\n",
    "    corp_bond_df.rename(columns = {'BAMLCC0A0CMTRIV': 'bond_ret'}, inplace = True)\n",
    "    stock_df = pd.merge(stock_df, corp_bond_df.bond_ret / 100, how = 'inner', left_index=True, right_index=True).drop_duplicates(keep='first')\n",
    "\n",
    "    # get U.S. Treasury yields from FRED\n",
    "    ten_year_treasury = pdr.DataReader('DGS10', 'fred', start = start_date, end = end_date, api_key = fred_api_key)\n",
    "    three_month_treasury = pdr.DataReader('DTB3', 'fred', start = start_date, end = end_date, api_key = fred_api_key)\n",
    "    treasury_df = pd.merge(ten_year_treasury, three_month_treasury, left_index=True, right_index=True, how = 'inner')\n",
    "    treasury_df['term_spread'] = treasury_df['DGS10'] - treasury_df['DTB3']\n",
    "    stock_df = pd.merge(stock_df, treasury_df.term_spread, how = 'inner', left_index=True, right_index=True).drop_duplicates(keep='first')\n",
    "\n",
    "    # get S&P 500 EPS from NASDAQ Data Link\n",
    "    spy_eps = dtalink.get('MULTPL/SP500_EARNINGS_MONTH', start_date = start_date, end_date = end_date)\n",
    "    spy_eps.rename(columns = {'Value': 'EPS'}, inplace = True)\n",
    "    stock_df = pd.merge(stock_df, spy_eps, how = 'left', left_index=True, right_index=True).drop_duplicates(keep='first')\n",
    "    stock_df.EPS = stock_df.EPS.fillna(method = 'ffill', axis = 0)\n",
    "\n",
    "    # get corporate bond spread from FRED\n",
    "    credit_spread_df = pdr.DataReader('BAA10YM', 'fred', start = start_date, end = end_date, api_key = fred_api_key)\n",
    "    credit_spread_df.rename(columns = {'BAA10YM': 'credit_spread'}, inplace = True)\n",
    "    stock_df = pd.merge(stock_df, credit_spread_df, how = 'left', left_index=True, right_index=True).drop_duplicates(keep='first')\n",
    "    stock_df.credit_spread = stock_df.credit_spread.fillna(method = 'ffill', axis = 0)\n",
    "\n",
    "    # get FRED 10-year inflation expectation\n",
    "    inflation_df = pdr.DataReader('T10YIE', 'fred', start = start_date, end = end_date, api_key = fred_api_key)\n",
    "    stock_df = pd.merge(stock_df, inflation_df, how = 'inner', left_index=True, right_index=True).drop_duplicates(keep='first').rename_axis('date').reset_index()\n",
    "\n",
    "    # save data to a CSV file\n",
    "    stock_df.to_csv(os.path.join(out_dir, '%s.csv' % ticker), index = False, header = True)\n",
    "    return stock_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Start timing\n",
    "    start_time = time.time()  \n",
    "    \n",
    "    # ##### Download data\n",
    "    # ticker = 'SPY'\n",
    "    ticker = 'BTC-USD'\n",
    "    start_date = '2010-01-01'\n",
    "    end_date = '2022-09-01'\n",
    "\n",
    "    out_dir = 'e:/Dropbox/Codes/ASRock-X99/Forecast_Stocks/Data/BTC-USD/'\n",
    "\n",
    "    stock_df = get_stock_data(ticker, start_date, end_date, out_dir=out_dir)\n",
    "    stock_df = stock_df.drop_duplicates(subset = ['date'], keep='first').reset_index(drop = True)\n",
    "    stock_df.Volume = stock_df.Volume.astype(np.float64)\n",
    "    \n",
    "    ##### Read the downloaded data into a dataframe\n",
    "    stock_df = pd.read_csv(os.path.join(out_dir, '%s.csv' % ticker), encoding = 'utf-8', header = 0, skiprows = 0, skipinitialspace = True, parse_dates=['date'])\n",
    "    display(stock_df.head())\n",
    "    sp500_pe_df = pd.read_csv(os.path.join(out_dir, 'SP500_PE.csv'), encoding = 'utf-8', header = 0, skiprows = 0, skipinitialspace = True, parse_dates=['date'])\n",
    "    \n",
    "    # merge all dataframes\n",
    "    stock_df = pd.merge(stock_df, sp500_pe_df, how = 'left', on = 'date').drop_duplicates(keep='first')\n",
    "    stock_df['sp500_pe'] = stock_df['sp500_pe'].fillna(method = 'bfill', axis = 0)\n",
    "    display(stock_df.head(90))\n",
    "    \n",
    "\n",
    "    # Compute the technical indicators\n",
    "    columns_to_drop = stock_df.columns[1:].tolist()\n",
    "    stock_dfs = []\n",
    "    stock_dfs.append(stock_df.loc[:, ~stock_df.columns.isin(['Open',\t'High',\t'Low',\t'Close',\t'Volume',\t'Dividends',\t'Stock Splits'])])\n",
    "\n",
    "    timeperiods = [14, 24, 34, 54, 104]\n",
    "    for timeperiod in timeperiods:\n",
    "        df = get_tech_indicators(stock_df, ticker, timeperiod, out_dir=out_dir)\n",
    "        df.drop(columns = columns_to_drop, axis = 1, inplace = True)\n",
    "        stock_dfs.append(df)\n",
    "\n",
    "    # Merge all the dataframes and remove duplicate columns\n",
    "    df_final = reduce(lambda left, right: pd.merge( left, right, on='date', suffixes=('', '_DROP') ).filter(regex='^(?!.*_DROP)'), stock_dfs) \n",
    "    df_final.insert(loc = 1, column = 'direction', value = (stock_df['return'] > 0).astype('int'))\n",
    "\n",
    "    df_final.dropna(inplace = True)\n",
    "    df_final = df_final.drop_duplicates(keep='first').reset_index(drop = True)\n",
    "    df_final = df_final.loc[:, ~df_final.T.duplicated(keep='first')] # remove duplicate columns\n",
    "\n",
    "    df_final.to_csv(os.path.join(out_dir, '%s_all_vars.csv' % ticker), index = False, header = True)\n",
    "    display( df_final.head() )\n",
    "\n",
    "    # Compute the pattern recognition indicators\n",
    "    stock_df = get_pattern_recognitions(stock_df, ticker, out_dir=out_dir)\n",
    "    df_final = pd.merge(df_final.set_index('date'), stock_df.set_index('date').loc[:, 'CDL2CROWS':], how = 'left', left_index = True, right_index = True)\n",
    "    df_final.dropna(inplace = True)\n",
    "    df_final.to_csv(os.path.join(out_dir, '%s_all_vars_plus_patterns.csv' % ticker), index = True, header = True)\n",
    "    display( df_final.head() )\n",
    "\n",
    "    print( 'Completed in: %s sec'%(time.time() - start_time) )\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
